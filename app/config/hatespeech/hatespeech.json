{
    "start-prompt" : "Lasst uns den Schei√ü analysieren!üí©",
    "result" : "Klassifiziert als: {prediction} ({certainty:.0%})",
    "introduction" : [
        "Du hast jetzt die M√∂glichkeit eine Nachricht an den Bot zu schreiben. Er wird dir antworten und versuchen dir eine Bewertung zur√ºck zu geben (eine Wahrscheinlichkeit) wie sehr deine Nachricht aggressiv/hasserf√ºllt ist oder nicht. Nichts was du schreibst wird gespeichert oder ver√∂ffentlicht."
    ],
    "commands": {
        "prompt" : [ "prompt", "PROMPT"]
    },
    "final-prompt" : "Ich bin am Ende. Du kannst jetzt wieder dem AVATAR_IN folgen!",
    "prompts" : [        
        "Eine der bemerkenswerten F√§higkeiten von Transformatormodellen ist ihre begrenzte F√§higkeit, Neologismen (neue W√∂rter) zu verarbeiten. Das ist interessant, weil Slang und verschl√ºsselte Sprache extrem effektiv sind, um naive Filtertechniken zu √ºberwinden. Versuche, in einer Beleidigung echte W√∂rter durch erfundene zu ersetzen",
        "Ein Problem mit den Datens√§tzen, die zum Trainieren von Hatespeech-Modellen verwendet werden, ist, dass es schwierig ist, W√∂rter zu unterscheiden, die eine starke Assoziation mit negativer Sprache haben. Zum Beispiel f√ºhren Synonyme f√ºr Homosexualit√§t oft dazu, dass der Text unabh√§ngig vom Kontext als beleidigend eingestuft wird. Versuche, nette Dinge √ºber homosexuelle Menschen zu sagen und schaue, wo das Modell Probleme hat.",
        "Das Ersetzen von W√∂rtern ist eine g√§ngige Methode, um das Verhalten des Modells zu testen. Nehme einen eindeutig liebevollen Satz, z. B. 'Ich liebe dich', und versuche, ein Wort nach dem anderen zu ersetzen. Was passiert wenn du den Satz immer um etwas negatives erweiterst? Bspw.: 'Ich liebe dich nicht'",
        "Sprachmodelle weisen oft geschlechtsspezifische Vorurteile auf. Versuche, das Geschlecht des Subjekts in einem Satz zu √§ndern. Erh√§ltst du ein anderes Verhalten?",
        "Sentiment- und Emotionserkennung sind verwandte Forschungsgebiete, die sich ebenfalls als fruchtbar erwiesen haben. In diesem Modell erwarten wir, dass emotionale Signifikanten einen gro√üen Einfluss auf das Ergebnis haben. Wie wirkt sich der Ausdruck von positiven und negativen Emotionen in Bezug auf eine Zielgruppe aus?",
        "Leider versteht dieses Modell nur regul√§re Zeichen und einige Satzzeichen. Dies wird zunehmend als Vers√§umnis angesehen, da Emojis ein wichtiger Bestandteil der Online-Kommunikation sind. Es werden Methoden entwickelt, um dies auszugleichen. Aber auch die Zeichensetzung ist wichtig. Versuche, mehr Ausrufezeichen in deinen Text einzubauen. Macht das einen Unterschied?",
        "Beeinflusst eine ‚Äúschlechte‚Äù Rechtschreibung das Ergebnis? K√∂nnte das der Fall sein?",
        "Schreibe den schlechtesten Witz, den du kennst. Mach schon, es schaut ja keiner zu!",
        "Viele Forscher:innen in diesem Bereich unterscheiden zwischen direkter und indirekter Beleidigung. Unserer Erfahrung nach ist direkter Antisemitismus online eher selten zu beobachten, aber indirekter Antisemitismus ist weit verbreitet, durch Tropen und Verschw√∂rungstmythen √ºber das internationale Bankwesen, Kabalen der Eliten und 'Hot-takes' zum Israel-Pal√§stina-Konflikt. Wie reagiert das Modell auf indirekte rassistische Argumente?",
        "Dieses Modell hat kein Verst√§ndnis f√ºr den Kontext jenseits des Textes, mit dem es gef√ºttert wird. Versuchen Sie, eine Aussage mit und ohne einige zus√§tzliche Informationen zu schreiben. Was wird ben√∂tigt, um ein Ergebnis zu √§ndern?",
        "Denke an etwas, das du gesagt hast oder zu dir gesagt wurde, dass du jetzt als hasserf√ºllt betrachtest. Das k√∂nnen Ausdr√ºcke sein, die du heute bereust gesagt zu haben. Stimmt das Modell mit dem alten Du oder dem neuen Du √ºberein?"
    ]
}